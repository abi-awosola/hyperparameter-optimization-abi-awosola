---
title: "COCS 5557: Practical Machine Learning"
subtitle: "Hyperparammeter Optimization"
format:
  pdf:
    include-in-header: 
      text: |
        \usepackage{lscape}
        \newcommand{\blandscape}{\begin{landscape}}
        \newcommand{\elandscape}{\end{landscape}}
jupyter: python3

geometry: "bottom=30mm"
---




**Introduction**

In this study, we aim to optimize the `hyperparameters` of various machine learning algorithms to achieve the best predictive performance on the `White Wine Quality` dataset. We will explore different algorithms and their `hyperparameter` configurations, aiming to find the optimal combination through `hyperparameter` optimization techniques. The primary goal is to demonstrate the effectiveness of `hyperparameter` tuning in improving model performance.


**Hyperparameter Tuning**

`Hyperparameters` are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning.

A model `hyperparameter` is a configuration that is external to the model and whose value cannot be estimated from data. 

`Hyperparameter tuning` consists of finding a set of optimal `hyperparameter` values for a learning algorithm while applying this optimized algorithm to any data set. That combination of `hyperparameters` maximizes the modelâ€™s performance, minimizing a predefined loss function to produce better results with fewer errors.

`Hyperparameter tuning` is an essential part of machine learning.

Optimizing hyperparameters is essential for improving model performance, preventing overfitting, enhancing generalization, and maximizing the effectiveness of machine learning algorithms in real-world applications. It is a critical step in the model development process that can lead to more accurate and reliable predictions.

`Hyperparameters` are different from parameters, which are the internal coefficients or weights for a model found by the learning algorithm. Unlike parameters, `hyperparameters` are specified when implementing the model.

Typically, it is challenging to know what values to use for the `hyperparameters` of a given algorithm on a given dataset, therefore it is common to use random or grid search strategies for different `hyperparameter` values.

The more `hyperparameters` of an algorithm that need to be tuned, the slower the tuning process would be. Therefore, it is desirable to select a minimum subset of model `hyperparameters` to search or tune.

Not all model `hyperparameters` are equally important. Some `hyperparameters` have an outsized effect on the behavior, and in turn, on the performance of a machine learning algorithm.

It is important to know which `hyperparameters` to focus on to get a good result in a timely manner.

In this Exercise, it will be discovered those `hyperparameters` that are most important for some selected machine learning classification algorithms.

Here are the classification algorithms that would be explored

1. Logistic Regression

2. Decision Tree Clasifier

3. Ridge Classifier

4. Gradient Boosting Classifier

5. Random Forest


We will consider these algorithms in the context of their scikit-learn implementation (in Python). However, the same `hyperparameter` suggestions are usable in the context of other platforms like Weka and R, as well.

 
**HPO Using Optuna**

Optuna is a popular Python library for automating HPO tasks efficiently.

Optuna uses an optimization engine to search for the best set of `hyperparameters` within a given search space. It employs various sampling algorithms such as `TPE` (`Tree-structured Parzen Estimator`) to explore the hyperparameter space effectively.

**How Optuna works with TPE to arrive at Hyperparameter Optimization**

1. *Define Search Space*: Before starting the optimization process, it is required to define the search space and the range for the hyperparameters to be optimized.

2. *Objective Function*: We define an objective function that takes the hyperparameters as input and returns a value to be maximized. The objective function represents the performance of the machine learning model.

3. *TPE Sampling*: Optuna uses the TPE algorithm to efficiently search the hyperparameter space. TPE maintains two probability density functions (PDFs) for each hyperparameter: one for the good values that improve the objective and one for the bad values. It constructs a tree of conditional probability distributions and samples new hyperparameter values based on the Bayesian optimization approach.

4. *Evaluate Objective*: Optuna evaluates the objective function using the sampled hyperparameters and collects the resulting objective values.

5. *Update Distributions*: Based on the evaluation results, TPE updates the PDFs for each hyperparameter. The PDFs are refined to focus on regions of the hyperparameter space that are likely to yield better performance.

6. *Repeat*: Steps 3-5 are repeated for a predefined number of trials, 100 in this case.

7. *Best Hyperparameters*: Once the optimization process is complete, Optuna returns the best set of hyperparameters found during the search based on the objective function's evaluations.

Overall, Optuna with TPE efficiently explores the hyperparameter space by iteratively sampling promising regions, updating probability distributions based on evaluation results, and converging towards optimal hyperparameter configurations. This iterative process helps in finding hyperparameters that yield the best performance for the given objective function.


Furthermore, 100 number of trials was used. Attempt was made to use 200 number of trials, but this did not give a significant improvement.

```{python}
#| echo: false

import sklearn
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import optuna
from optuna.samplers import TPESampler


# Load the white wine quality dataset
white_wine_data = pd.read_csv(r'C:\Users\Laptop\OneDrive\Desktop\winequality-white.csv', sep=r';')

# Split the data into features and target
X = white_wine_data.drop(columns=['quality'])
y = white_wine_data['quality']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data preprocessing
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```


```{python}
#| echo: false
#To store results of models, we create two dictionaries
result_dict_HPO = {}
result_dict_test = {}

#Suppresses intermediate output
optuna.logging.set_verbosity(optuna.logging.ERROR) 
```



**Logistic Regression**

*Hyperparameter search space:*

`C = trial.suggest_float("C", 0.1, 10)`

`penalty = trial.suggest_categorical("penalty", [None, "l2"])`


```{python}
#| echo: false
import pandas as pd
import warnings
from sklearn.linear_model import LogisticRegression


def objective(trial):
    # Define hyperparameters to be optimized
    C = trial.suggest_float('C', 1e-5, 1e5, log=True)
    penalty = trial.suggest_categorical('penalty', [None, 'l2'])
    # Initialize Logistic Regression classifier with hyperparameters
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        clf = LogisticRegression(C=C, penalty=penalty, random_state=42)
        clf.fit(X_train_scaled, y_train)
        val_acc = clf.score(X_test_scaled, y_test)
    return val_acc

# Create Optuna study with TPESampler
study = optuna.create_study(direction='maximize', sampler=TPESampler())
study.optimize(objective, n_trials=100)

best_trial = study.best_value
print(f"Best trial accuracy: {best_trial}")
print("Parameters for the best trial are:")
for key, value in study.best_trial.params.items():
    print(f"{key}: {value}")



# Reshapes y_train into a 1D array

import numpy as np

y_train = np.ravel(y_train)

# Scale the input features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)


from sklearn.linear_model import LogisticRegression

model1 = LogisticRegression(max_iter=1000)

model1.fit(X_train,y_train)
model1.score(X_test,y_test)

#Store results in the dictionaries
result_dict_HPO["Logistic Score w/ HPO"] = best_trial
result_dict_test["Logistic Score w/o HPO"] = model1.score(X_test,y_test)
```


**Decision Tree Classifier**

*Hyperparameter search space:*

`criterion = trial.suggest_categorical("criterion", ["gini", "entropy"])`
    
`max_depth = trial.suggest_int("max_depth", 3, 15)`
    
`min_samples_split = trial.suggest_int("min_samples_split", 2, 20)`
    
`min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 10)`


```{python}
#| echo: false




from sklearn.tree import DecisionTreeClassifier
from optuna.samplers import TPESampler

def objective(trial):
    # Define hyperparameters to be optimized
    max_depth = trial.suggest_int('max_depth', 1, 32)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)
    
    # Initialize Decision Tree classifier with hyperparameters
    clf = DecisionTreeClassifier(max_depth=max_depth,
                                  min_samples_split=min_samples_split,
                                  min_samples_leaf=min_samples_leaf,
                                  random_state=42)
    
    clf.fit(X_train_scaled, y_train)
    val_acc = clf.score(X_test_scaled, y_test)
    return val_acc

# Create Optuna study with TPESampler
study = optuna.create_study(direction='maximize', sampler=TPESampler())
study.optimize(objective, n_trials=100)

best_trial = study.best_value
print(f"Best trial accuracy: {best_trial}")
print("Parameters for the best trial are:")
for key, value in study.best_trial.params.items():
    print(f"{key}: {value}")


# define model
model2 = DecisionTreeClassifier()


model2.fit(X_train,y_train)
model2.score(X_test,y_test)

#Store results in the dictionaries
result_dict_HPO["Decision Tree Score w/ HPO"] = best_trial
result_dict_test["Decision Tree Score w/o HPO"] = model2.score(X_test,y_test)
```


**Ridge Classifier**

*Hyperparameter search space:*

`alpha = trial.suggest_float('alpha', 0.01, 10.0)`


```{python}
#| echo: false



from sklearn.linear_model import RidgeClassifier
from optuna.samplers import TPESampler



def objective(trial):
    # Define hyperparameters to be optimized
    alpha = trial.suggest_float('alpha', 1e-5, 1e5, log=True)
    # Initialize Ridge classifier with hyperparameters
    clf = RidgeClassifier(alpha=alpha, random_state=42)
    clf.fit(X_train_scaled, y_train)
    y_pred = clf.predict(X_test_scaled)
    val_acc = accuracy_score(y_test, y_pred)
    return val_acc

# Create Optuna study with TPESampler
study = optuna.create_study(direction='maximize', sampler=TPESampler())
study.optimize(objective, n_trials=100)

best_trial = study.best_value
print(f"Best trial accuracy: {best_trial}")
print("Parameters for the best trial are:")
for key, value in study.best_trial.params.items():
    print(f"{key}: {value}")


# define model
model3 = RidgeClassifier()


model3.fit(X_train,y_train)
model3.score(X_test,y_test)


#Store results in the dictionaries
result_dict_HPO["Ridge Classifier Score w/ HPO"] = best_trial
result_dict_test["Ridge Classifier Score w/o HPO"] = model3.score(X_test,y_test)
```





**Gradient Boosting Classifier**

*Hyperparameter search space:*

`n_estimators = trial.suggest_int('n_estimators', 10, 100)`

`learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)`

`max_depth = trial.suggest_int('max_depth', 1, 10)`
    
`min_samples_split = trial.suggest_int('min_samples_split', 2, 20)`
    
`min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)`


```{python}
#| echo: false



from sklearn.ensemble import GradientBoostingClassifier
from optuna.samplers import TPESampler

def objective(trial):
    # Define hyperparameters to be optimized
    n_estimators = trial.suggest_int('n_estimators', 10, 100)
    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)
    max_depth = trial.suggest_int('max_depth', 1, 10)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)
    
    # Initialize Gradient Boosting classifier with hyperparameters
    clf = GradientBoostingClassifier(n_estimators=n_estimators, 
                                     learning_rate=learning_rate, 
                                     max_depth=max_depth, 
                                     min_samples_split=min_samples_split, 
                                     min_samples_leaf=min_samples_leaf, 
                                     random_state=42)
    
    clf.fit(X_train_scaled, y_train)
    val_acc = clf.score(X_test_scaled, y_test)
    return val_acc

# Create Optuna study with TPESampler
study = optuna.create_study(direction='maximize', sampler=TPESampler())
study.optimize(objective, n_trials=100)

best_trial = study.best_value
print(f"Best trial accuracy: {best_trial}")
print("Parameters for the best trial are:")
for key, value in study.best_trial.params.items():
    print(f"{key}: {value}")


model4 = GradientBoostingClassifier()

model4.fit(X_train,y_train)
model4.score(X_test,y_test)


#Store results in the dictionaries
result_dict_HPO["Gradient Boosting Score w/ HPO"] = best_trial
result_dict_test["Gradient Boosting Score w/o HPO"] = model4.score(X_test,y_test)
```







**Random Forest Classifier**

*Hyperparameter search space:*

`n_estimators = trial.suggest_int('n_estimators', 10, 100)`

`max_depth = trial.suggest_int('max_depth', 1, 32)`

`min_samples_split = trial.suggest_int('min_samples_split', 2, 20)`

`min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)`


```{python}
#| echo: false


def objective_func(trial):
    # Random Forest hyperparameters
    n_estimators = trial.suggest_int("n_estimators", 10, 1000, log=True)
    max_depth = trial.suggest_int("max_depth", 1, 32)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 20)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 20)
    
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42
    )

    clf.fit(X_train_scaled, y_train)
    val_acc = clf.score(X_test_scaled, y_test)
    return val_acc

# Create Optuna study with TPESampler
study = optuna.create_study(direction='maximize', sampler=TPESampler())
study.optimize(objective_func, n_trials=100)

best_trial = study.best_trial.value
print(f"Best trial accuracy: {best_trial}")
print("Parameters for the best trial are:")
for key, value in study.best_trial.params.items():
    print(f"{key}: {value}")


model5 = RandomForestClassifier()

model5.fit(X_train,y_train)
model5.score(X_test,y_test)


#Store results in the dictionaries
result_dict_HPO["Random Forest Score w/ HPO"] = best_trial
result_dict_test["random Forest Score w/o HPO"] = model5.score(X_test,y_test) 


```




```{python}
#| echo: false

df_result_HPO = pd.DataFrame.from_dict(result_dict_HPO,orient = "index", columns=["Score"])
df_result_HPO

```


```{python}
#| echo: false

df_result_test = pd.DataFrame.from_dict(result_dict_test,orient = "index",columns=["Score"])
df_result_test
```



**Visualizing the scores**

```{python}
#| echo: false
#| message: false
#| warning: false

import seaborn as sns
import matplotlib.pyplot as plt

# Define custom color palette
colors = ["navy", "maroon", "darkgreen", "dimgrey", "blue"]


fig, ax = plt.subplots(1, 2, figsize=(35, 30))

# Plot for training data set
sns.barplot(x=df_result_HPO.index, y=df_result_HPO.Score, ax=ax[0], palette=colors, hue=df_result_test.index, legend=False)

# Plot for test data set
sns.barplot(x=df_result_test.index, y=df_result_test.Score, ax=ax[1], palette=colors, hue=df_result_test.index, legend=False)

# Set ticks and labels with rotation
ax[0].set_xticks(range(len(df_result_HPO.index)))
ax[0].set_xticklabels(df_result_HPO.index, rotation=45, ha='right', fontsize=20)
ax[1].set_xticks(range(len(df_result_test.index)))
ax[1].set_xticklabels(df_result_test.index, rotation=45, ha='right', fontsize=20)


ax[0].set_xlabel('ML Algorithm w/ HPO Score', fontsize=20) 
ax[0].set_ylabel('Score', fontsize=20)  
ax[1].set_xlabel('ML Algorithm w/o HPO Score', fontsize=20)  
ax[1].set_ylabel('Score', fontsize=20)  

# Increases the size of y-axis markings
ax[0].tick_params(axis='y', labelsize=20) 
ax[1].tick_params(axis='y', labelsize=20)  


plt.show()

```


**Conclusion**

After tuning the `hyperparameters`, although Gradient Boosting Classifier algorithm is the most improved machine learning algorithm with  
Best trial accuracy: 0.7010204081632653
Parameters for the best trial are: n_estimators: 97
                                   learning_rate: 0.0439
                                   max_depth: 10
                                   min_samples_split: 20
                                   min_samples_leaf: 8` `hyperparameters` combination, for predicting the white wine `quality`; the Random Forest algorithm is still the best algorithm with: 
Best trial accuracy: 0.7081632653061225
Parameters for the best trial are: n_estimators: 304
                                   max_depth: 19
                                   min_samples_split: 2
                                   min_samples_leaf: 1.


**Code**

The code used for this Exerecise will be provided in a separate Quarto Document file for reproducibility.